{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os,PIL,pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "#设置GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/bird_photos'\n",
    "data_dir = pathlib.Path(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "566"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count = len(list(data_dir.glob('*/*')))\n",
    "image_count # 这里输出的是图片的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/bird_photos'\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize([224,224]),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 566\n",
       "    Root location: ./data/bird_photos\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = datasets.ImageFolder(data_dir,transform=data_transform)\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(total_data))\n",
    "test_size = len(total_data) - train_size\n",
    "train_data,test_data = torch.utils.data.random_split(total_data,[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 114)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size,test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dl = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.utils.checkpoint as cp\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Module):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate,\n",
    "                        kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        \n",
    "        self.add_module('SE_Block',SE_Block(growth_rate, reduction=16))\n",
    "        self.drop_rate = drop_rate\n",
    "        self.efficient = efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.SE_Block(self.conv2(self.relu2(self.norm2(bottleneck_output))))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                efficient=efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "            \n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "class SE_Block(nn.Module):\n",
    "    def __init__(self, ch_in, reduction=16):\n",
    "        super(SE_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # 全局自适应池化\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(ch_in, ch_in // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(ch_in // reduction, ch_in, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c) # squeeze操作\n",
    "        y = self.fc(y).view(b, c, 1, 1) # FC获取通道注意力权重，是具有全局信息的\n",
    "        return x * y.expand_as(x) # 注意力作用每一个通道上\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate, block_config,num_init_features=24, compression=0.5, bn_size=4, drop_rate=0,\n",
    "                 num_classes=10, small_inputs=True, efficient=False):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "        assert 0 < compression <= 1, 'compression of densenet should be between 0 and 1'\n",
    "\n",
    "        # First convolution\n",
    "        if small_inputs:\n",
    "            self.features = nn.Sequential(OrderedDict([\n",
    "                ('conv0', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "            ]))\n",
    "        else:\n",
    "            self.features = nn.Sequential(OrderedDict([\n",
    "                ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
    "            ]))\n",
    "            self.features.add_module('norm0', nn.BatchNorm2d(num_init_features))\n",
    "            self.features.add_module('relu0', nn.ReLU(inplace=True))\n",
    "            self.features.add_module('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1,\n",
    "                                                           ceil_mode=False))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                efficient=efficient,\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=int(num_features * compression))\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = int(num_features * compression)\n",
    "            #self.features.add_module('SE_Block%d' % (i + 1),SE_Block(num_features, reduction=16))\n",
    "    \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm_final', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape:  torch.Size([2, 4])\n",
      "tensor([[ 0.0213, -0.2534, -0.1534, -0.2042],\n",
      "        [ 0.0160, -0.2537, -0.1431, -0.2145]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义完成，测试一下\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "model = DenseNet(growth_rate=32, block_config=(6,12,24,16), compression=0.5,\n",
    " num_init_features=64, bn_size=4, drop_rate=0.2,num_classes=4,efficient=True)\n",
    "out = model(x)\n",
    "print('out.shape: ', out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4        [-1, 128, 224, 224]           8,192\n",
      "       BatchNorm2d-5        [-1, 128, 224, 224]             256\n",
      "              ReLU-6        [-1, 128, 224, 224]               0\n",
      "            Conv2d-7         [-1, 32, 224, 224]          36,864\n",
      " AdaptiveAvgPool2d-8             [-1, 32, 1, 1]               0\n",
      "            Linear-9                    [-1, 2]              64\n",
      "             ReLU-10                    [-1, 2]               0\n",
      "           Linear-11                   [-1, 32]              64\n",
      "          Sigmoid-12                   [-1, 32]               0\n",
      "         SE_Block-13         [-1, 32, 224, 224]               0\n",
      "      _DenseLayer-14         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-15         [-1, 96, 224, 224]             192\n",
      "             ReLU-16         [-1, 96, 224, 224]               0\n",
      "           Conv2d-17        [-1, 128, 224, 224]          12,288\n",
      "      BatchNorm2d-18        [-1, 128, 224, 224]             256\n",
      "             ReLU-19        [-1, 128, 224, 224]               0\n",
      "           Conv2d-20         [-1, 32, 224, 224]          36,864\n",
      "AdaptiveAvgPool2d-21             [-1, 32, 1, 1]               0\n",
      "           Linear-22                    [-1, 2]              64\n",
      "             ReLU-23                    [-1, 2]               0\n",
      "           Linear-24                   [-1, 32]              64\n",
      "          Sigmoid-25                   [-1, 32]               0\n",
      "         SE_Block-26         [-1, 32, 224, 224]               0\n",
      "      _DenseLayer-27         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-28        [-1, 128, 224, 224]             256\n",
      "             ReLU-29        [-1, 128, 224, 224]               0\n",
      "           Conv2d-30        [-1, 128, 224, 224]          16,384\n",
      "      BatchNorm2d-31        [-1, 128, 224, 224]             256\n",
      "             ReLU-32        [-1, 128, 224, 224]               0\n",
      "           Conv2d-33         [-1, 32, 224, 224]          36,864\n",
      "AdaptiveAvgPool2d-34             [-1, 32, 1, 1]               0\n",
      "           Linear-35                    [-1, 2]              64\n",
      "             ReLU-36                    [-1, 2]               0\n",
      "           Linear-37                   [-1, 32]              64\n",
      "          Sigmoid-38                   [-1, 32]               0\n",
      "         SE_Block-39         [-1, 32, 224, 224]               0\n",
      "      _DenseLayer-40         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-41        [-1, 160, 224, 224]             320\n",
      "             ReLU-42        [-1, 160, 224, 224]               0\n",
      "           Conv2d-43        [-1, 128, 224, 224]          20,480\n",
      "      BatchNorm2d-44        [-1, 128, 224, 224]             256\n",
      "             ReLU-45        [-1, 128, 224, 224]               0\n",
      "           Conv2d-46         [-1, 32, 224, 224]          36,864\n",
      "AdaptiveAvgPool2d-47             [-1, 32, 1, 1]               0\n",
      "           Linear-48                    [-1, 2]              64\n",
      "             ReLU-49                    [-1, 2]               0\n",
      "           Linear-50                   [-1, 32]              64\n",
      "          Sigmoid-51                   [-1, 32]               0\n",
      "         SE_Block-52         [-1, 32, 224, 224]               0\n",
      "      _DenseLayer-53         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-54        [-1, 192, 224, 224]             384\n",
      "             ReLU-55        [-1, 192, 224, 224]               0\n",
      "           Conv2d-56        [-1, 128, 224, 224]          24,576\n",
      "      BatchNorm2d-57        [-1, 128, 224, 224]             256\n",
      "             ReLU-58        [-1, 128, 224, 224]               0\n",
      "           Conv2d-59         [-1, 32, 224, 224]          36,864\n",
      "AdaptiveAvgPool2d-60             [-1, 32, 1, 1]               0\n",
      "           Linear-61                    [-1, 2]              64\n",
      "             ReLU-62                    [-1, 2]               0\n",
      "           Linear-63                   [-1, 32]              64\n",
      "          Sigmoid-64                   [-1, 32]               0\n",
      "         SE_Block-65         [-1, 32, 224, 224]               0\n",
      "      _DenseLayer-66         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-67        [-1, 224, 224, 224]             448\n",
      "             ReLU-68        [-1, 224, 224, 224]               0\n",
      "           Conv2d-69        [-1, 128, 224, 224]          28,672\n",
      "      BatchNorm2d-70        [-1, 128, 224, 224]             256\n",
      "             ReLU-71        [-1, 128, 224, 224]               0\n",
      "           Conv2d-72         [-1, 32, 224, 224]          36,864\n",
      "AdaptiveAvgPool2d-73             [-1, 32, 1, 1]               0\n",
      "           Linear-74                    [-1, 2]              64\n",
      "             ReLU-75                    [-1, 2]               0\n",
      "           Linear-76                   [-1, 32]              64\n",
      "          Sigmoid-77                   [-1, 32]               0\n",
      "         SE_Block-78         [-1, 32, 224, 224]               0\n",
      "      _DenseLayer-79         [-1, 32, 224, 224]               0\n",
      "      _DenseBlock-80        [-1, 256, 224, 224]               0\n",
      "      BatchNorm2d-81        [-1, 256, 224, 224]             512\n",
      "             ReLU-82        [-1, 256, 224, 224]               0\n",
      "           Conv2d-83        [-1, 128, 224, 224]          32,768\n",
      "        AvgPool2d-84        [-1, 128, 112, 112]               0\n",
      "      BatchNorm2d-85        [-1, 128, 112, 112]             256\n",
      "             ReLU-86        [-1, 128, 112, 112]               0\n",
      "           Conv2d-87        [-1, 128, 112, 112]          16,384\n",
      "      BatchNorm2d-88        [-1, 128, 112, 112]             256\n",
      "             ReLU-89        [-1, 128, 112, 112]               0\n",
      "           Conv2d-90         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-91             [-1, 32, 1, 1]               0\n",
      "           Linear-92                    [-1, 2]              64\n",
      "             ReLU-93                    [-1, 2]               0\n",
      "           Linear-94                   [-1, 32]              64\n",
      "          Sigmoid-95                   [-1, 32]               0\n",
      "         SE_Block-96         [-1, 32, 112, 112]               0\n",
      "      _DenseLayer-97         [-1, 32, 112, 112]               0\n",
      "      BatchNorm2d-98        [-1, 160, 112, 112]             320\n",
      "             ReLU-99        [-1, 160, 112, 112]               0\n",
      "          Conv2d-100        [-1, 128, 112, 112]          20,480\n",
      "     BatchNorm2d-101        [-1, 128, 112, 112]             256\n",
      "            ReLU-102        [-1, 128, 112, 112]               0\n",
      "          Conv2d-103         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-104             [-1, 32, 1, 1]               0\n",
      "          Linear-105                    [-1, 2]              64\n",
      "            ReLU-106                    [-1, 2]               0\n",
      "          Linear-107                   [-1, 32]              64\n",
      "         Sigmoid-108                   [-1, 32]               0\n",
      "        SE_Block-109         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-110         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-111        [-1, 192, 112, 112]             384\n",
      "            ReLU-112        [-1, 192, 112, 112]               0\n",
      "          Conv2d-113        [-1, 128, 112, 112]          24,576\n",
      "     BatchNorm2d-114        [-1, 128, 112, 112]             256\n",
      "            ReLU-115        [-1, 128, 112, 112]               0\n",
      "          Conv2d-116         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-117             [-1, 32, 1, 1]               0\n",
      "          Linear-118                    [-1, 2]              64\n",
      "            ReLU-119                    [-1, 2]               0\n",
      "          Linear-120                   [-1, 32]              64\n",
      "         Sigmoid-121                   [-1, 32]               0\n",
      "        SE_Block-122         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-123         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-124        [-1, 224, 112, 112]             448\n",
      "            ReLU-125        [-1, 224, 112, 112]               0\n",
      "          Conv2d-126        [-1, 128, 112, 112]          28,672\n",
      "     BatchNorm2d-127        [-1, 128, 112, 112]             256\n",
      "            ReLU-128        [-1, 128, 112, 112]               0\n",
      "          Conv2d-129         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-130             [-1, 32, 1, 1]               0\n",
      "          Linear-131                    [-1, 2]              64\n",
      "            ReLU-132                    [-1, 2]               0\n",
      "          Linear-133                   [-1, 32]              64\n",
      "         Sigmoid-134                   [-1, 32]               0\n",
      "        SE_Block-135         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-136         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-137        [-1, 256, 112, 112]             512\n",
      "            ReLU-138        [-1, 256, 112, 112]               0\n",
      "          Conv2d-139        [-1, 128, 112, 112]          32,768\n",
      "     BatchNorm2d-140        [-1, 128, 112, 112]             256\n",
      "            ReLU-141        [-1, 128, 112, 112]               0\n",
      "          Conv2d-142         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-143             [-1, 32, 1, 1]               0\n",
      "          Linear-144                    [-1, 2]              64\n",
      "            ReLU-145                    [-1, 2]               0\n",
      "          Linear-146                   [-1, 32]              64\n",
      "         Sigmoid-147                   [-1, 32]               0\n",
      "        SE_Block-148         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-149         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-150        [-1, 288, 112, 112]             576\n",
      "            ReLU-151        [-1, 288, 112, 112]               0\n",
      "          Conv2d-152        [-1, 128, 112, 112]          36,864\n",
      "     BatchNorm2d-153        [-1, 128, 112, 112]             256\n",
      "            ReLU-154        [-1, 128, 112, 112]               0\n",
      "          Conv2d-155         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-156             [-1, 32, 1, 1]               0\n",
      "          Linear-157                    [-1, 2]              64\n",
      "            ReLU-158                    [-1, 2]               0\n",
      "          Linear-159                   [-1, 32]              64\n",
      "         Sigmoid-160                   [-1, 32]               0\n",
      "        SE_Block-161         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-162         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-163        [-1, 320, 112, 112]             640\n",
      "            ReLU-164        [-1, 320, 112, 112]               0\n",
      "          Conv2d-165        [-1, 128, 112, 112]          40,960\n",
      "     BatchNorm2d-166        [-1, 128, 112, 112]             256\n",
      "            ReLU-167        [-1, 128, 112, 112]               0\n",
      "          Conv2d-168         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-169             [-1, 32, 1, 1]               0\n",
      "          Linear-170                    [-1, 2]              64\n",
      "            ReLU-171                    [-1, 2]               0\n",
      "          Linear-172                   [-1, 32]              64\n",
      "         Sigmoid-173                   [-1, 32]               0\n",
      "        SE_Block-174         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-175         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-176        [-1, 352, 112, 112]             704\n",
      "            ReLU-177        [-1, 352, 112, 112]               0\n",
      "          Conv2d-178        [-1, 128, 112, 112]          45,056\n",
      "     BatchNorm2d-179        [-1, 128, 112, 112]             256\n",
      "            ReLU-180        [-1, 128, 112, 112]               0\n",
      "          Conv2d-181         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-182             [-1, 32, 1, 1]               0\n",
      "          Linear-183                    [-1, 2]              64\n",
      "            ReLU-184                    [-1, 2]               0\n",
      "          Linear-185                   [-1, 32]              64\n",
      "         Sigmoid-186                   [-1, 32]               0\n",
      "        SE_Block-187         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-188         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-189        [-1, 384, 112, 112]             768\n",
      "            ReLU-190        [-1, 384, 112, 112]               0\n",
      "          Conv2d-191        [-1, 128, 112, 112]          49,152\n",
      "     BatchNorm2d-192        [-1, 128, 112, 112]             256\n",
      "            ReLU-193        [-1, 128, 112, 112]               0\n",
      "          Conv2d-194         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-195             [-1, 32, 1, 1]               0\n",
      "          Linear-196                    [-1, 2]              64\n",
      "            ReLU-197                    [-1, 2]               0\n",
      "          Linear-198                   [-1, 32]              64\n",
      "         Sigmoid-199                   [-1, 32]               0\n",
      "        SE_Block-200         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-201         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-202        [-1, 416, 112, 112]             832\n",
      "            ReLU-203        [-1, 416, 112, 112]               0\n",
      "          Conv2d-204        [-1, 128, 112, 112]          53,248\n",
      "     BatchNorm2d-205        [-1, 128, 112, 112]             256\n",
      "            ReLU-206        [-1, 128, 112, 112]               0\n",
      "          Conv2d-207         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-208             [-1, 32, 1, 1]               0\n",
      "          Linear-209                    [-1, 2]              64\n",
      "            ReLU-210                    [-1, 2]               0\n",
      "          Linear-211                   [-1, 32]              64\n",
      "         Sigmoid-212                   [-1, 32]               0\n",
      "        SE_Block-213         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-214         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-215        [-1, 448, 112, 112]             896\n",
      "            ReLU-216        [-1, 448, 112, 112]               0\n",
      "          Conv2d-217        [-1, 128, 112, 112]          57,344\n",
      "     BatchNorm2d-218        [-1, 128, 112, 112]             256\n",
      "            ReLU-219        [-1, 128, 112, 112]               0\n",
      "          Conv2d-220         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-221             [-1, 32, 1, 1]               0\n",
      "          Linear-222                    [-1, 2]              64\n",
      "            ReLU-223                    [-1, 2]               0\n",
      "          Linear-224                   [-1, 32]              64\n",
      "         Sigmoid-225                   [-1, 32]               0\n",
      "        SE_Block-226         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-227         [-1, 32, 112, 112]               0\n",
      "     BatchNorm2d-228        [-1, 480, 112, 112]             960\n",
      "            ReLU-229        [-1, 480, 112, 112]               0\n",
      "          Conv2d-230        [-1, 128, 112, 112]          61,440\n",
      "     BatchNorm2d-231        [-1, 128, 112, 112]             256\n",
      "            ReLU-232        [-1, 128, 112, 112]               0\n",
      "          Conv2d-233         [-1, 32, 112, 112]          36,864\n",
      "AdaptiveAvgPool2d-234             [-1, 32, 1, 1]               0\n",
      "          Linear-235                    [-1, 2]              64\n",
      "            ReLU-236                    [-1, 2]               0\n",
      "          Linear-237                   [-1, 32]              64\n",
      "         Sigmoid-238                   [-1, 32]               0\n",
      "        SE_Block-239         [-1, 32, 112, 112]               0\n",
      "     _DenseLayer-240         [-1, 32, 112, 112]               0\n",
      "     _DenseBlock-241        [-1, 512, 112, 112]               0\n",
      "     BatchNorm2d-242        [-1, 512, 112, 112]           1,024\n",
      "            ReLU-243        [-1, 512, 112, 112]               0\n",
      "          Conv2d-244        [-1, 256, 112, 112]         131,072\n",
      "       AvgPool2d-245          [-1, 256, 56, 56]               0\n",
      "     BatchNorm2d-246          [-1, 256, 56, 56]             512\n",
      "            ReLU-247          [-1, 256, 56, 56]               0\n",
      "          Conv2d-248          [-1, 128, 56, 56]          32,768\n",
      "     BatchNorm2d-249          [-1, 128, 56, 56]             256\n",
      "            ReLU-250          [-1, 128, 56, 56]               0\n",
      "          Conv2d-251           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-252             [-1, 32, 1, 1]               0\n",
      "          Linear-253                    [-1, 2]              64\n",
      "            ReLU-254                    [-1, 2]               0\n",
      "          Linear-255                   [-1, 32]              64\n",
      "         Sigmoid-256                   [-1, 32]               0\n",
      "        SE_Block-257           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-258           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-259          [-1, 288, 56, 56]             576\n",
      "            ReLU-260          [-1, 288, 56, 56]               0\n",
      "          Conv2d-261          [-1, 128, 56, 56]          36,864\n",
      "     BatchNorm2d-262          [-1, 128, 56, 56]             256\n",
      "            ReLU-263          [-1, 128, 56, 56]               0\n",
      "          Conv2d-264           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-265             [-1, 32, 1, 1]               0\n",
      "          Linear-266                    [-1, 2]              64\n",
      "            ReLU-267                    [-1, 2]               0\n",
      "          Linear-268                   [-1, 32]              64\n",
      "         Sigmoid-269                   [-1, 32]               0\n",
      "        SE_Block-270           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-271           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-272          [-1, 320, 56, 56]             640\n",
      "            ReLU-273          [-1, 320, 56, 56]               0\n",
      "          Conv2d-274          [-1, 128, 56, 56]          40,960\n",
      "     BatchNorm2d-275          [-1, 128, 56, 56]             256\n",
      "            ReLU-276          [-1, 128, 56, 56]               0\n",
      "          Conv2d-277           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-278             [-1, 32, 1, 1]               0\n",
      "          Linear-279                    [-1, 2]              64\n",
      "            ReLU-280                    [-1, 2]               0\n",
      "          Linear-281                   [-1, 32]              64\n",
      "         Sigmoid-282                   [-1, 32]               0\n",
      "        SE_Block-283           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-284           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-285          [-1, 352, 56, 56]             704\n",
      "            ReLU-286          [-1, 352, 56, 56]               0\n",
      "          Conv2d-287          [-1, 128, 56, 56]          45,056\n",
      "     BatchNorm2d-288          [-1, 128, 56, 56]             256\n",
      "            ReLU-289          [-1, 128, 56, 56]               0\n",
      "          Conv2d-290           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-291             [-1, 32, 1, 1]               0\n",
      "          Linear-292                    [-1, 2]              64\n",
      "            ReLU-293                    [-1, 2]               0\n",
      "          Linear-294                   [-1, 32]              64\n",
      "         Sigmoid-295                   [-1, 32]               0\n",
      "        SE_Block-296           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-297           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-298          [-1, 384, 56, 56]             768\n",
      "            ReLU-299          [-1, 384, 56, 56]               0\n",
      "          Conv2d-300          [-1, 128, 56, 56]          49,152\n",
      "     BatchNorm2d-301          [-1, 128, 56, 56]             256\n",
      "            ReLU-302          [-1, 128, 56, 56]               0\n",
      "          Conv2d-303           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-304             [-1, 32, 1, 1]               0\n",
      "          Linear-305                    [-1, 2]              64\n",
      "            ReLU-306                    [-1, 2]               0\n",
      "          Linear-307                   [-1, 32]              64\n",
      "         Sigmoid-308                   [-1, 32]               0\n",
      "        SE_Block-309           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-310           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-311          [-1, 416, 56, 56]             832\n",
      "            ReLU-312          [-1, 416, 56, 56]               0\n",
      "          Conv2d-313          [-1, 128, 56, 56]          53,248\n",
      "     BatchNorm2d-314          [-1, 128, 56, 56]             256\n",
      "            ReLU-315          [-1, 128, 56, 56]               0\n",
      "          Conv2d-316           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-317             [-1, 32, 1, 1]               0\n",
      "          Linear-318                    [-1, 2]              64\n",
      "            ReLU-319                    [-1, 2]               0\n",
      "          Linear-320                   [-1, 32]              64\n",
      "         Sigmoid-321                   [-1, 32]               0\n",
      "        SE_Block-322           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-323           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-324          [-1, 448, 56, 56]             896\n",
      "            ReLU-325          [-1, 448, 56, 56]               0\n",
      "          Conv2d-326          [-1, 128, 56, 56]          57,344\n",
      "     BatchNorm2d-327          [-1, 128, 56, 56]             256\n",
      "            ReLU-328          [-1, 128, 56, 56]               0\n",
      "          Conv2d-329           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-330             [-1, 32, 1, 1]               0\n",
      "          Linear-331                    [-1, 2]              64\n",
      "            ReLU-332                    [-1, 2]               0\n",
      "          Linear-333                   [-1, 32]              64\n",
      "         Sigmoid-334                   [-1, 32]               0\n",
      "        SE_Block-335           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-336           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-337          [-1, 480, 56, 56]             960\n",
      "            ReLU-338          [-1, 480, 56, 56]               0\n",
      "          Conv2d-339          [-1, 128, 56, 56]          61,440\n",
      "     BatchNorm2d-340          [-1, 128, 56, 56]             256\n",
      "            ReLU-341          [-1, 128, 56, 56]               0\n",
      "          Conv2d-342           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-343             [-1, 32, 1, 1]               0\n",
      "          Linear-344                    [-1, 2]              64\n",
      "            ReLU-345                    [-1, 2]               0\n",
      "          Linear-346                   [-1, 32]              64\n",
      "         Sigmoid-347                   [-1, 32]               0\n",
      "        SE_Block-348           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-349           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-350          [-1, 512, 56, 56]           1,024\n",
      "            ReLU-351          [-1, 512, 56, 56]               0\n",
      "          Conv2d-352          [-1, 128, 56, 56]          65,536\n",
      "     BatchNorm2d-353          [-1, 128, 56, 56]             256\n",
      "            ReLU-354          [-1, 128, 56, 56]               0\n",
      "          Conv2d-355           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-356             [-1, 32, 1, 1]               0\n",
      "          Linear-357                    [-1, 2]              64\n",
      "            ReLU-358                    [-1, 2]               0\n",
      "          Linear-359                   [-1, 32]              64\n",
      "         Sigmoid-360                   [-1, 32]               0\n",
      "        SE_Block-361           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-362           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-363          [-1, 544, 56, 56]           1,088\n",
      "            ReLU-364          [-1, 544, 56, 56]               0\n",
      "          Conv2d-365          [-1, 128, 56, 56]          69,632\n",
      "     BatchNorm2d-366          [-1, 128, 56, 56]             256\n",
      "            ReLU-367          [-1, 128, 56, 56]               0\n",
      "          Conv2d-368           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-369             [-1, 32, 1, 1]               0\n",
      "          Linear-370                    [-1, 2]              64\n",
      "            ReLU-371                    [-1, 2]               0\n",
      "          Linear-372                   [-1, 32]              64\n",
      "         Sigmoid-373                   [-1, 32]               0\n",
      "        SE_Block-374           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-375           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-376          [-1, 576, 56, 56]           1,152\n",
      "            ReLU-377          [-1, 576, 56, 56]               0\n",
      "          Conv2d-378          [-1, 128, 56, 56]          73,728\n",
      "     BatchNorm2d-379          [-1, 128, 56, 56]             256\n",
      "            ReLU-380          [-1, 128, 56, 56]               0\n",
      "          Conv2d-381           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-382             [-1, 32, 1, 1]               0\n",
      "          Linear-383                    [-1, 2]              64\n",
      "            ReLU-384                    [-1, 2]               0\n",
      "          Linear-385                   [-1, 32]              64\n",
      "         Sigmoid-386                   [-1, 32]               0\n",
      "        SE_Block-387           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-388           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-389          [-1, 608, 56, 56]           1,216\n",
      "            ReLU-390          [-1, 608, 56, 56]               0\n",
      "          Conv2d-391          [-1, 128, 56, 56]          77,824\n",
      "     BatchNorm2d-392          [-1, 128, 56, 56]             256\n",
      "            ReLU-393          [-1, 128, 56, 56]               0\n",
      "          Conv2d-394           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-395             [-1, 32, 1, 1]               0\n",
      "          Linear-396                    [-1, 2]              64\n",
      "            ReLU-397                    [-1, 2]               0\n",
      "          Linear-398                   [-1, 32]              64\n",
      "         Sigmoid-399                   [-1, 32]               0\n",
      "        SE_Block-400           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-401           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-402          [-1, 640, 56, 56]           1,280\n",
      "            ReLU-403          [-1, 640, 56, 56]               0\n",
      "          Conv2d-404          [-1, 128, 56, 56]          81,920\n",
      "     BatchNorm2d-405          [-1, 128, 56, 56]             256\n",
      "            ReLU-406          [-1, 128, 56, 56]               0\n",
      "          Conv2d-407           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-408             [-1, 32, 1, 1]               0\n",
      "          Linear-409                    [-1, 2]              64\n",
      "            ReLU-410                    [-1, 2]               0\n",
      "          Linear-411                   [-1, 32]              64\n",
      "         Sigmoid-412                   [-1, 32]               0\n",
      "        SE_Block-413           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-414           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-415          [-1, 672, 56, 56]           1,344\n",
      "            ReLU-416          [-1, 672, 56, 56]               0\n",
      "          Conv2d-417          [-1, 128, 56, 56]          86,016\n",
      "     BatchNorm2d-418          [-1, 128, 56, 56]             256\n",
      "            ReLU-419          [-1, 128, 56, 56]               0\n",
      "          Conv2d-420           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-421             [-1, 32, 1, 1]               0\n",
      "          Linear-422                    [-1, 2]              64\n",
      "            ReLU-423                    [-1, 2]               0\n",
      "          Linear-424                   [-1, 32]              64\n",
      "         Sigmoid-425                   [-1, 32]               0\n",
      "        SE_Block-426           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-427           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-428          [-1, 704, 56, 56]           1,408\n",
      "            ReLU-429          [-1, 704, 56, 56]               0\n",
      "          Conv2d-430          [-1, 128, 56, 56]          90,112\n",
      "     BatchNorm2d-431          [-1, 128, 56, 56]             256\n",
      "            ReLU-432          [-1, 128, 56, 56]               0\n",
      "          Conv2d-433           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-434             [-1, 32, 1, 1]               0\n",
      "          Linear-435                    [-1, 2]              64\n",
      "            ReLU-436                    [-1, 2]               0\n",
      "          Linear-437                   [-1, 32]              64\n",
      "         Sigmoid-438                   [-1, 32]               0\n",
      "        SE_Block-439           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-440           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-441          [-1, 736, 56, 56]           1,472\n",
      "            ReLU-442          [-1, 736, 56, 56]               0\n",
      "          Conv2d-443          [-1, 128, 56, 56]          94,208\n",
      "     BatchNorm2d-444          [-1, 128, 56, 56]             256\n",
      "            ReLU-445          [-1, 128, 56, 56]               0\n",
      "          Conv2d-446           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-447             [-1, 32, 1, 1]               0\n",
      "          Linear-448                    [-1, 2]              64\n",
      "            ReLU-449                    [-1, 2]               0\n",
      "          Linear-450                   [-1, 32]              64\n",
      "         Sigmoid-451                   [-1, 32]               0\n",
      "        SE_Block-452           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-453           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-454          [-1, 768, 56, 56]           1,536\n",
      "            ReLU-455          [-1, 768, 56, 56]               0\n",
      "          Conv2d-456          [-1, 128, 56, 56]          98,304\n",
      "     BatchNorm2d-457          [-1, 128, 56, 56]             256\n",
      "            ReLU-458          [-1, 128, 56, 56]               0\n",
      "          Conv2d-459           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-460             [-1, 32, 1, 1]               0\n",
      "          Linear-461                    [-1, 2]              64\n",
      "            ReLU-462                    [-1, 2]               0\n",
      "          Linear-463                   [-1, 32]              64\n",
      "         Sigmoid-464                   [-1, 32]               0\n",
      "        SE_Block-465           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-466           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-467          [-1, 800, 56, 56]           1,600\n",
      "            ReLU-468          [-1, 800, 56, 56]               0\n",
      "          Conv2d-469          [-1, 128, 56, 56]         102,400\n",
      "     BatchNorm2d-470          [-1, 128, 56, 56]             256\n",
      "            ReLU-471          [-1, 128, 56, 56]               0\n",
      "          Conv2d-472           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-473             [-1, 32, 1, 1]               0\n",
      "          Linear-474                    [-1, 2]              64\n",
      "            ReLU-475                    [-1, 2]               0\n",
      "          Linear-476                   [-1, 32]              64\n",
      "         Sigmoid-477                   [-1, 32]               0\n",
      "        SE_Block-478           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-479           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-480          [-1, 832, 56, 56]           1,664\n",
      "            ReLU-481          [-1, 832, 56, 56]               0\n",
      "          Conv2d-482          [-1, 128, 56, 56]         106,496\n",
      "     BatchNorm2d-483          [-1, 128, 56, 56]             256\n",
      "            ReLU-484          [-1, 128, 56, 56]               0\n",
      "          Conv2d-485           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-486             [-1, 32, 1, 1]               0\n",
      "          Linear-487                    [-1, 2]              64\n",
      "            ReLU-488                    [-1, 2]               0\n",
      "          Linear-489                   [-1, 32]              64\n",
      "         Sigmoid-490                   [-1, 32]               0\n",
      "        SE_Block-491           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-492           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-493          [-1, 864, 56, 56]           1,728\n",
      "            ReLU-494          [-1, 864, 56, 56]               0\n",
      "          Conv2d-495          [-1, 128, 56, 56]         110,592\n",
      "     BatchNorm2d-496          [-1, 128, 56, 56]             256\n",
      "            ReLU-497          [-1, 128, 56, 56]               0\n",
      "          Conv2d-498           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-499             [-1, 32, 1, 1]               0\n",
      "          Linear-500                    [-1, 2]              64\n",
      "            ReLU-501                    [-1, 2]               0\n",
      "          Linear-502                   [-1, 32]              64\n",
      "         Sigmoid-503                   [-1, 32]               0\n",
      "        SE_Block-504           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-505           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-506          [-1, 896, 56, 56]           1,792\n",
      "            ReLU-507          [-1, 896, 56, 56]               0\n",
      "          Conv2d-508          [-1, 128, 56, 56]         114,688\n",
      "     BatchNorm2d-509          [-1, 128, 56, 56]             256\n",
      "            ReLU-510          [-1, 128, 56, 56]               0\n",
      "          Conv2d-511           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-512             [-1, 32, 1, 1]               0\n",
      "          Linear-513                    [-1, 2]              64\n",
      "            ReLU-514                    [-1, 2]               0\n",
      "          Linear-515                   [-1, 32]              64\n",
      "         Sigmoid-516                   [-1, 32]               0\n",
      "        SE_Block-517           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-518           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-519          [-1, 928, 56, 56]           1,856\n",
      "            ReLU-520          [-1, 928, 56, 56]               0\n",
      "          Conv2d-521          [-1, 128, 56, 56]         118,784\n",
      "     BatchNorm2d-522          [-1, 128, 56, 56]             256\n",
      "            ReLU-523          [-1, 128, 56, 56]               0\n",
      "          Conv2d-524           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-525             [-1, 32, 1, 1]               0\n",
      "          Linear-526                    [-1, 2]              64\n",
      "            ReLU-527                    [-1, 2]               0\n",
      "          Linear-528                   [-1, 32]              64\n",
      "         Sigmoid-529                   [-1, 32]               0\n",
      "        SE_Block-530           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-531           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-532          [-1, 960, 56, 56]           1,920\n",
      "            ReLU-533          [-1, 960, 56, 56]               0\n",
      "          Conv2d-534          [-1, 128, 56, 56]         122,880\n",
      "     BatchNorm2d-535          [-1, 128, 56, 56]             256\n",
      "            ReLU-536          [-1, 128, 56, 56]               0\n",
      "          Conv2d-537           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-538             [-1, 32, 1, 1]               0\n",
      "          Linear-539                    [-1, 2]              64\n",
      "            ReLU-540                    [-1, 2]               0\n",
      "          Linear-541                   [-1, 32]              64\n",
      "         Sigmoid-542                   [-1, 32]               0\n",
      "        SE_Block-543           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-544           [-1, 32, 56, 56]               0\n",
      "     BatchNorm2d-545          [-1, 992, 56, 56]           1,984\n",
      "            ReLU-546          [-1, 992, 56, 56]               0\n",
      "          Conv2d-547          [-1, 128, 56, 56]         126,976\n",
      "     BatchNorm2d-548          [-1, 128, 56, 56]             256\n",
      "            ReLU-549          [-1, 128, 56, 56]               0\n",
      "          Conv2d-550           [-1, 32, 56, 56]          36,864\n",
      "AdaptiveAvgPool2d-551             [-1, 32, 1, 1]               0\n",
      "          Linear-552                    [-1, 2]              64\n",
      "            ReLU-553                    [-1, 2]               0\n",
      "          Linear-554                   [-1, 32]              64\n",
      "         Sigmoid-555                   [-1, 32]               0\n",
      "        SE_Block-556           [-1, 32, 56, 56]               0\n",
      "     _DenseLayer-557           [-1, 32, 56, 56]               0\n",
      "     _DenseBlock-558         [-1, 1024, 56, 56]               0\n",
      "     BatchNorm2d-559         [-1, 1024, 56, 56]           2,048\n",
      "            ReLU-560         [-1, 1024, 56, 56]               0\n",
      "          Conv2d-561          [-1, 512, 56, 56]         524,288\n",
      "       AvgPool2d-562          [-1, 512, 28, 28]               0\n",
      "     BatchNorm2d-563          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-564          [-1, 512, 28, 28]               0\n",
      "          Conv2d-565          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-566          [-1, 128, 28, 28]             256\n",
      "            ReLU-567          [-1, 128, 28, 28]               0\n",
      "          Conv2d-568           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-569             [-1, 32, 1, 1]               0\n",
      "          Linear-570                    [-1, 2]              64\n",
      "            ReLU-571                    [-1, 2]               0\n",
      "          Linear-572                   [-1, 32]              64\n",
      "         Sigmoid-573                   [-1, 32]               0\n",
      "        SE_Block-574           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-575           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-576          [-1, 544, 28, 28]           1,088\n",
      "            ReLU-577          [-1, 544, 28, 28]               0\n",
      "          Conv2d-578          [-1, 128, 28, 28]          69,632\n",
      "     BatchNorm2d-579          [-1, 128, 28, 28]             256\n",
      "            ReLU-580          [-1, 128, 28, 28]               0\n",
      "          Conv2d-581           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-582             [-1, 32, 1, 1]               0\n",
      "          Linear-583                    [-1, 2]              64\n",
      "            ReLU-584                    [-1, 2]               0\n",
      "          Linear-585                   [-1, 32]              64\n",
      "         Sigmoid-586                   [-1, 32]               0\n",
      "        SE_Block-587           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-588           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-589          [-1, 576, 28, 28]           1,152\n",
      "            ReLU-590          [-1, 576, 28, 28]               0\n",
      "          Conv2d-591          [-1, 128, 28, 28]          73,728\n",
      "     BatchNorm2d-592          [-1, 128, 28, 28]             256\n",
      "            ReLU-593          [-1, 128, 28, 28]               0\n",
      "          Conv2d-594           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-595             [-1, 32, 1, 1]               0\n",
      "          Linear-596                    [-1, 2]              64\n",
      "            ReLU-597                    [-1, 2]               0\n",
      "          Linear-598                   [-1, 32]              64\n",
      "         Sigmoid-599                   [-1, 32]               0\n",
      "        SE_Block-600           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-601           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-602          [-1, 608, 28, 28]           1,216\n",
      "            ReLU-603          [-1, 608, 28, 28]               0\n",
      "          Conv2d-604          [-1, 128, 28, 28]          77,824\n",
      "     BatchNorm2d-605          [-1, 128, 28, 28]             256\n",
      "            ReLU-606          [-1, 128, 28, 28]               0\n",
      "          Conv2d-607           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-608             [-1, 32, 1, 1]               0\n",
      "          Linear-609                    [-1, 2]              64\n",
      "            ReLU-610                    [-1, 2]               0\n",
      "          Linear-611                   [-1, 32]              64\n",
      "         Sigmoid-612                   [-1, 32]               0\n",
      "        SE_Block-613           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-614           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-615          [-1, 640, 28, 28]           1,280\n",
      "            ReLU-616          [-1, 640, 28, 28]               0\n",
      "          Conv2d-617          [-1, 128, 28, 28]          81,920\n",
      "     BatchNorm2d-618          [-1, 128, 28, 28]             256\n",
      "            ReLU-619          [-1, 128, 28, 28]               0\n",
      "          Conv2d-620           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-621             [-1, 32, 1, 1]               0\n",
      "          Linear-622                    [-1, 2]              64\n",
      "            ReLU-623                    [-1, 2]               0\n",
      "          Linear-624                   [-1, 32]              64\n",
      "         Sigmoid-625                   [-1, 32]               0\n",
      "        SE_Block-626           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-627           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-628          [-1, 672, 28, 28]           1,344\n",
      "            ReLU-629          [-1, 672, 28, 28]               0\n",
      "          Conv2d-630          [-1, 128, 28, 28]          86,016\n",
      "     BatchNorm2d-631          [-1, 128, 28, 28]             256\n",
      "            ReLU-632          [-1, 128, 28, 28]               0\n",
      "          Conv2d-633           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-634             [-1, 32, 1, 1]               0\n",
      "          Linear-635                    [-1, 2]              64\n",
      "            ReLU-636                    [-1, 2]               0\n",
      "          Linear-637                   [-1, 32]              64\n",
      "         Sigmoid-638                   [-1, 32]               0\n",
      "        SE_Block-639           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-640           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-641          [-1, 704, 28, 28]           1,408\n",
      "            ReLU-642          [-1, 704, 28, 28]               0\n",
      "          Conv2d-643          [-1, 128, 28, 28]          90,112\n",
      "     BatchNorm2d-644          [-1, 128, 28, 28]             256\n",
      "            ReLU-645          [-1, 128, 28, 28]               0\n",
      "          Conv2d-646           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-647             [-1, 32, 1, 1]               0\n",
      "          Linear-648                    [-1, 2]              64\n",
      "            ReLU-649                    [-1, 2]               0\n",
      "          Linear-650                   [-1, 32]              64\n",
      "         Sigmoid-651                   [-1, 32]               0\n",
      "        SE_Block-652           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-653           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-654          [-1, 736, 28, 28]           1,472\n",
      "            ReLU-655          [-1, 736, 28, 28]               0\n",
      "          Conv2d-656          [-1, 128, 28, 28]          94,208\n",
      "     BatchNorm2d-657          [-1, 128, 28, 28]             256\n",
      "            ReLU-658          [-1, 128, 28, 28]               0\n",
      "          Conv2d-659           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-660             [-1, 32, 1, 1]               0\n",
      "          Linear-661                    [-1, 2]              64\n",
      "            ReLU-662                    [-1, 2]               0\n",
      "          Linear-663                   [-1, 32]              64\n",
      "         Sigmoid-664                   [-1, 32]               0\n",
      "        SE_Block-665           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-666           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-667          [-1, 768, 28, 28]           1,536\n",
      "            ReLU-668          [-1, 768, 28, 28]               0\n",
      "          Conv2d-669          [-1, 128, 28, 28]          98,304\n",
      "     BatchNorm2d-670          [-1, 128, 28, 28]             256\n",
      "            ReLU-671          [-1, 128, 28, 28]               0\n",
      "          Conv2d-672           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-673             [-1, 32, 1, 1]               0\n",
      "          Linear-674                    [-1, 2]              64\n",
      "            ReLU-675                    [-1, 2]               0\n",
      "          Linear-676                   [-1, 32]              64\n",
      "         Sigmoid-677                   [-1, 32]               0\n",
      "        SE_Block-678           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-679           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-680          [-1, 800, 28, 28]           1,600\n",
      "            ReLU-681          [-1, 800, 28, 28]               0\n",
      "          Conv2d-682          [-1, 128, 28, 28]         102,400\n",
      "     BatchNorm2d-683          [-1, 128, 28, 28]             256\n",
      "            ReLU-684          [-1, 128, 28, 28]               0\n",
      "          Conv2d-685           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-686             [-1, 32, 1, 1]               0\n",
      "          Linear-687                    [-1, 2]              64\n",
      "            ReLU-688                    [-1, 2]               0\n",
      "          Linear-689                   [-1, 32]              64\n",
      "         Sigmoid-690                   [-1, 32]               0\n",
      "        SE_Block-691           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-692           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-693          [-1, 832, 28, 28]           1,664\n",
      "            ReLU-694          [-1, 832, 28, 28]               0\n",
      "          Conv2d-695          [-1, 128, 28, 28]         106,496\n",
      "     BatchNorm2d-696          [-1, 128, 28, 28]             256\n",
      "            ReLU-697          [-1, 128, 28, 28]               0\n",
      "          Conv2d-698           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-699             [-1, 32, 1, 1]               0\n",
      "          Linear-700                    [-1, 2]              64\n",
      "            ReLU-701                    [-1, 2]               0\n",
      "          Linear-702                   [-1, 32]              64\n",
      "         Sigmoid-703                   [-1, 32]               0\n",
      "        SE_Block-704           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-705           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-706          [-1, 864, 28, 28]           1,728\n",
      "            ReLU-707          [-1, 864, 28, 28]               0\n",
      "          Conv2d-708          [-1, 128, 28, 28]         110,592\n",
      "     BatchNorm2d-709          [-1, 128, 28, 28]             256\n",
      "            ReLU-710          [-1, 128, 28, 28]               0\n",
      "          Conv2d-711           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-712             [-1, 32, 1, 1]               0\n",
      "          Linear-713                    [-1, 2]              64\n",
      "            ReLU-714                    [-1, 2]               0\n",
      "          Linear-715                   [-1, 32]              64\n",
      "         Sigmoid-716                   [-1, 32]               0\n",
      "        SE_Block-717           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-718           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-719          [-1, 896, 28, 28]           1,792\n",
      "            ReLU-720          [-1, 896, 28, 28]               0\n",
      "          Conv2d-721          [-1, 128, 28, 28]         114,688\n",
      "     BatchNorm2d-722          [-1, 128, 28, 28]             256\n",
      "            ReLU-723          [-1, 128, 28, 28]               0\n",
      "          Conv2d-724           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-725             [-1, 32, 1, 1]               0\n",
      "          Linear-726                    [-1, 2]              64\n",
      "            ReLU-727                    [-1, 2]               0\n",
      "          Linear-728                   [-1, 32]              64\n",
      "         Sigmoid-729                   [-1, 32]               0\n",
      "        SE_Block-730           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-731           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-732          [-1, 928, 28, 28]           1,856\n",
      "            ReLU-733          [-1, 928, 28, 28]               0\n",
      "          Conv2d-734          [-1, 128, 28, 28]         118,784\n",
      "     BatchNorm2d-735          [-1, 128, 28, 28]             256\n",
      "            ReLU-736          [-1, 128, 28, 28]               0\n",
      "          Conv2d-737           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-738             [-1, 32, 1, 1]               0\n",
      "          Linear-739                    [-1, 2]              64\n",
      "            ReLU-740                    [-1, 2]               0\n",
      "          Linear-741                   [-1, 32]              64\n",
      "         Sigmoid-742                   [-1, 32]               0\n",
      "        SE_Block-743           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-744           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-745          [-1, 960, 28, 28]           1,920\n",
      "            ReLU-746          [-1, 960, 28, 28]               0\n",
      "          Conv2d-747          [-1, 128, 28, 28]         122,880\n",
      "     BatchNorm2d-748          [-1, 128, 28, 28]             256\n",
      "            ReLU-749          [-1, 128, 28, 28]               0\n",
      "          Conv2d-750           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-751             [-1, 32, 1, 1]               0\n",
      "          Linear-752                    [-1, 2]              64\n",
      "            ReLU-753                    [-1, 2]               0\n",
      "          Linear-754                   [-1, 32]              64\n",
      "         Sigmoid-755                   [-1, 32]               0\n",
      "        SE_Block-756           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-757           [-1, 32, 28, 28]               0\n",
      "     BatchNorm2d-758          [-1, 992, 28, 28]           1,984\n",
      "            ReLU-759          [-1, 992, 28, 28]               0\n",
      "          Conv2d-760          [-1, 128, 28, 28]         126,976\n",
      "     BatchNorm2d-761          [-1, 128, 28, 28]             256\n",
      "            ReLU-762          [-1, 128, 28, 28]               0\n",
      "          Conv2d-763           [-1, 32, 28, 28]          36,864\n",
      "AdaptiveAvgPool2d-764             [-1, 32, 1, 1]               0\n",
      "          Linear-765                    [-1, 2]              64\n",
      "            ReLU-766                    [-1, 2]               0\n",
      "          Linear-767                   [-1, 32]              64\n",
      "         Sigmoid-768                   [-1, 32]               0\n",
      "        SE_Block-769           [-1, 32, 28, 28]               0\n",
      "     _DenseLayer-770           [-1, 32, 28, 28]               0\n",
      "     _DenseBlock-771         [-1, 1024, 28, 28]               0\n",
      "     BatchNorm2d-772         [-1, 1024, 28, 28]           2,048\n",
      "          Linear-773                    [-1, 4]           4,100\n",
      "================================================================\n",
      "Total params: 6,957,572\n",
      "Trainable params: 6,957,572\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 4854.11\n",
      "Params size (MB): 26.54\n",
      "Estimated Total Size (MB): 4881.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "# 统计模型参数量以及其他指标\n",
    "import torchsummary as summary\n",
    "summary.summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)  # 训练集的大小\n",
    "    num_batches = len(dataloader)   # 批次数目, (size/batch_size，向上取整)\n",
    "\n",
    "    train_loss, train_acc = 0, 0  # 初始化训练损失和正确率\n",
    "    \n",
    "    for X, y in dataloader:  # 获取图片及其标签\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 计算预测误差\n",
    "        pred = model(X)          # 网络输出\n",
    "        loss = loss_fn(pred, y)  # 计算网络输出和真实值之间的差距，targets为真实值，计算二者差值即为损失\n",
    "        \n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()  # grad属性归零\n",
    "        loss.backward()        # 反向传播\n",
    "        optimizer.step()       # 每一步自动更新\n",
    "        \n",
    "        # 记录acc与loss\n",
    "        train_acc  += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "    train_acc  /= size\n",
    "    train_loss /= num_batches\n",
    "\n",
    "    return train_acc, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test (dataloader, model, loss_fn):\n",
    "    size        = len(dataloader.dataset)  # 测试集的大小\n",
    "    num_batches = len(dataloader)          # 批次数目, (size/batch_size，向上取整)\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # 当不进行训练时，停止梯度更新，节省计算内存消耗\n",
    "    with torch.no_grad():\n",
    "        for imgs, target in dataloader:\n",
    "            imgs, target = imgs.to(device), target.to(device)\n",
    "            \n",
    "            # 计算loss\n",
    "            target_pred = model(imgs)\n",
    "            loss        = loss_fn(target_pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            test_acc  += (target_pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "    test_acc  /= size\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train_acc:58.2%, Train_loss:1.001, Test_acc:65.8%, Test_loss:0.938, Lr:1.00E-04\n",
      "Epoch: 2, Train_acc:72.1%, Train_loss:0.816, Test_acc:74.6%, Test_loss:0.871, Lr:1.00E-04\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr= 1e-4)\n",
    "loss_fn    = nn.CrossEntropyLoss() # 创建损失函数\n",
    "\n",
    "epochs     = 10\n",
    "\n",
    "train_loss = []\n",
    "train_acc  = []\n",
    "test_loss  = []\n",
    "test_acc   = []\n",
    "\n",
    "best_acc = 0    # 设置一个最佳准确率，作为最佳模型的判别指标\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_train_acc, epoch_train_loss = train(train_dl, model, loss_fn, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_test_acc, epoch_test_loss = test(test_dl, model, loss_fn)\n",
    "    \n",
    "    # 保存最佳模型到 best_model\n",
    "    if epoch_test_acc > best_acc:\n",
    "        best_acc   = epoch_test_acc\n",
    "        best_model = copy.deepcopy(model)\n",
    "    \n",
    "    train_acc.append(epoch_train_acc)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    test_acc.append(epoch_test_acc)\n",
    "    test_loss.append(epoch_test_loss)\n",
    "    \n",
    "    # 获取当前的学习率\n",
    "    lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    \n",
    "    template = ('Epoch:{:2d}, Train_acc:{:.1f}%, Train_loss:{:.3f}, Test_acc:{:.1f}%, Test_loss:{:.3f}, Lr:{:.2E}')\n",
    "    print(template.format(epoch+1, epoch_train_acc*100, epoch_train_loss, \n",
    "                          epoch_test_acc*100, epoch_test_loss, lr))\n",
    "    \n",
    "# 保存最佳模型到文件中\n",
    "# PATH = './best_model.pth'  # 保存的参数文件名\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
