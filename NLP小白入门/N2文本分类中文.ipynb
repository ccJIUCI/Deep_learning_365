{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv('train.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = Data.iloc[:,0].str.split('\\t',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = Data.rename(columns={0:'sentence',1:'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_lst = list(Data['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label2idx = {label_lst[i]:i for i in range(len(label_lst))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_label_2_idx(x):\n",
    "    return label2idx[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data['label'] = Data['label'].apply(transform_label_2_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./Bert_Model/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,Data):\n",
    "        super().__init__()\n",
    "        self.data = Data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 加载bert的基础分词器\n",
    "        self.TEXT_LEN = max(self.data.iloc[:,0].apply(len)) # 记录训练集当中的最长句子\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        label,text = self.data.iloc[index,1],self.data.iloc[index,0] # 按照索引读取类别和正文\n",
    "        tokened = self.tokenizer(text)\n",
    "        input_ids = tokened['input_ids']\n",
    "        mask = tokened['attention_mask']\n",
    "        BERT_PAD_ID = self.tokenizer.pad_token_id\n",
    "        if len(input_ids) < self.TEXT_LEN:\n",
    "            pad_len = (self.TEXT_LEN - len(input_ids))\n",
    "            input_ids += [BERT_PAD_ID] * pad_len\n",
    "            mask += [0] * pad_len\n",
    "        target = int(label)\n",
    "        return torch.tensor(input_ids[:self.TEXT_LEN]), torch.tensor(mask[:self.TEXT_LEN]), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 768\n",
    "NUM_FILTERS = 256\n",
    "NUM_CLASSES = 12\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('./Bert_Model/bert-base-uncased')\n",
    "        for name ,param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, NUM_FILTERS, (i, EMBEDDING_DIM)) for i in FILTER_SIZES])\n",
    "        self.linear = nn.Linear(NUM_FILTERS * 3, NUM_CLASSES)\n",
    "\n",
    "    def conv_and_pool(self, conv, input):\n",
    "        out = conv(input)\n",
    "        out = F.relu(out)\n",
    "        return F.max_pool2d(out, (out.shape[2], out.shape[3])).squeeze()\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        out = self.bert(input, mask)[0].unsqueeze(1)\n",
    "        out = torch.cat([self.conv_and_pool(conv, out) for conv in self.convs], dim=1)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 311kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 2.52kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 317kB/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(Data)\n",
    "spilt_train,split_valid =  random_split(train_dataset,[int(len(train_dataset)*0.8),len(train_dataset)-int(len(train_dataset)*0.8)])\n",
    "train_loader = data.DataLoader(spilt_train, batch_size=10, shuffle=True)\n",
    "dev_loader = data.DataLoader(split_valid, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Bert_Model/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10\n",
    "LR = 1e-3\n",
    "model = TextCNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1.0,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(pred, true, target_names=None, output_dict=False):\n",
    "    return classification_report(\n",
    "        true,\n",
    "        pred,\n",
    "        target_names=target_names,\n",
    "        output_dict=output_dict,\n",
    "        zero_division=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = './OUTPUT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 0 batch: 0 loss: 2.3414 train_acc: 0.2 dev_acc: 0.1\n",
      ">> epoch: 0 batch: 500 loss: 2.11987 train_acc: 0.3 dev_acc: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:39<05:54, 39.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 1 batch: 0 loss: 0.9267 train_acc: 0.7 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 500 loss: 1.53099 train_acc: 0.5 dev_acc: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:13<04:50, 36.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 2 batch: 0 loss: 0.94175 train_acc: 0.6 dev_acc: 0.5\n",
      ">> epoch: 2 batch: 500 loss: 1.08657 train_acc: 0.6 dev_acc: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:47<04:07, 35.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 3 batch: 0 loss: 0.87594 train_acc: 0.7 dev_acc: 0.4\n",
      ">> epoch: 3 batch: 500 loss: 0.54531 train_acc: 0.8 dev_acc: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:21<03:29, 34.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 4 batch: 0 loss: 0.61071 train_acc: 0.7 dev_acc: 0.7\n",
      ">> epoch: 4 batch: 500 loss: 1.03577 train_acc: 0.6 dev_acc: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:55<02:53, 34.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 5 batch: 0 loss: 0.97601 train_acc: 0.6 dev_acc: 0.8\n",
      ">> epoch: 5 batch: 500 loss: 0.71175 train_acc: 0.7 dev_acc: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:30<02:17, 34.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 6 batch: 0 loss: 1.0109 train_acc: 0.7 dev_acc: 0.8\n",
      ">> epoch: 6 batch: 500 loss: 0.38359 train_acc: 0.8 dev_acc: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:04<01:43, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 7 batch: 0 loss: 0.55266 train_acc: 0.8 dev_acc: 0.9\n",
      ">> epoch: 7 batch: 500 loss: 0.6743 train_acc: 0.8 dev_acc: 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:38<01:08, 34.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 8 batch: 0 loss: 0.11041 train_acc: 1.0 dev_acc: 0.8\n",
      ">> epoch: 8 batch: 500 loss: 0.40319 train_acc: 0.8 dev_acc: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:12<00:34, 34.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 9 batch: 0 loss: 0.18773 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 9 batch: 500 loss: 0.72395 train_acc: 0.7 dev_acc: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:47<00:00, 34.71s/it]\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(EPOCH)):\n",
    "    for b, (input, mask, target) in enumerate(train_loader):\n",
    "        input = input.to(DEVICE)\n",
    "        mask = mask.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        pred = model(input, mask)\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 500 != 0:\n",
    "            continue\n",
    "\n",
    "        y_pred = torch.argmax(pred, dim=1)\n",
    "        report = evaluate(y_pred.cpu().data.numpy(), target.cpu().data.numpy(), output_dict=True)\n",
    "        with torch.no_grad():\n",
    "            for dev_input, dev_mask, dev_target in dev_loader:\n",
    "                dev_input = dev_input.to(DEVICE)\n",
    "                dev_mask = dev_mask.to(DEVICE)\n",
    "                dev_target = dev_target.to(DEVICE)\n",
    "                dev_pred = model(dev_input, dev_mask)\n",
    "                dev_pred_ = torch.argmax(dev_pred, dim=1)\n",
    "                dev_report = evaluate(dev_pred_.cpu().data.numpy(), dev_target.cpu().data.numpy(), output_dict=True)\n",
    "                break\n",
    "        print(\n",
    "            '>> epoch:', e,\n",
    "            'batch:', b,\n",
    "            'loss:', round(loss.item(), 5),\n",
    "            'train_acc:', report['accuracy'],\n",
    "            'dev_acc:', dev_report['accuracy']\n",
    "        )\n",
    "    if e%50 ==0:\n",
    "        torch.save(model, MODEL_DIR + f'{e}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
