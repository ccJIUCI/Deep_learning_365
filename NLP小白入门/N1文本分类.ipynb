{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e4165476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "596dc05e-6aeb-4665-b8a5-95b6bbbe2206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c86d33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv('./AG_NEWS/train.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92e3e9e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./Bert_Model/bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "660537f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,type='train'):\n",
    "        super().__init__()\n",
    "        if type == 'train':\n",
    "            self.data = pd.read_csv('./AG_NEWS/train.csv',header=None)\n",
    "        elif type == 'test':\n",
    "            self.data = pd.read_csv('./AG_NEWS/test.csv',header=None)\n",
    "        else:\n",
    "            raise Exception('type只能为train或者test')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 加载bert的基础分词器\n",
    "        self.TEXT_LEN = max(self.data.iloc[:][1].apply(len)) # 记录训练集当中的最长句子\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        label,text = self.data.iloc[index][0],self.data.iloc[index][1] # 按照索引读取类别和正文\n",
    "        tokened = self.tokenizer(text)\n",
    "        input_ids = tokened['input_ids']\n",
    "        mask = tokened['attention_mask']\n",
    "        BERT_PAD_ID = self.tokenizer.pad_token_id\n",
    "        if len(input_ids) < self.TEXT_LEN:\n",
    "            pad_len = (self.TEXT_LEN - len(input_ids))\n",
    "            input_ids += [BERT_PAD_ID] * pad_len\n",
    "            mask += [0] * pad_len\n",
    "        target = int(label - 1) #这里需要注意-1\n",
    "        return torch.tensor(input_ids[:self.TEXT_LEN]), torch.tensor(mask[:self.TEXT_LEN]), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecb78b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_label():\n",
    "    text = open('./AG_NEWS/classes.txt').read()\n",
    "    id2label = text.split()\n",
    "    return id2label, {v: k for k, v in enumerate(id2label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "659b30d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 768\n",
    "NUM_FILTERS = 256\n",
    "NUM_CLASSES = 4\n",
    "FILTER_SIZES = [2, 3, 4]\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('./Bert_Model/bert-base-uncased')\n",
    "        for name ,param in self.bert.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, NUM_FILTERS, (i, EMBEDDING_DIM)) for i in FILTER_SIZES])\n",
    "        self.linear = nn.Linear(NUM_FILTERS * 3, NUM_CLASSES)\n",
    "\n",
    "    def conv_and_pool(self, conv, input):\n",
    "        out = conv(input)\n",
    "        out = F.relu(out)\n",
    "        return F.max_pool2d(out, (out.shape[2], out.shape[3])).squeeze()\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "        out = self.bert(input, mask)[0].unsqueeze(1)\n",
    "        out = torch.cat([self.conv_and_pool(conv, out) for conv in self.convs], dim=1)\n",
    "        return self.linear(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0389cd02-e94e-4c55-8dc0-0921afe7957d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 下面开始进行模型的训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8da3b7e0-b9cc-438e-beee-ebcc4a237157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66f6e712-20eb-46c0-8c5d-be21176cb87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label, _ = get_label()\n",
    "train_dataset = Dataset('train')\n",
    "spilt_train,split_valid =  random_split(train_dataset,[int(len(train_dataset)*0.8),len(train_dataset)-int(len(train_dataset)*0.8)])\n",
    "train_loader = data.DataLoader(spilt_train, batch_size=10, shuffle=True)\n",
    "dev_loader = data.DataLoader(split_valid, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "73073299-4249-4243-aa52-b6b8d358a78e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Bert_Model/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 3\n",
    "LR = 1e-3\n",
    "model = TextCNN().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,1.0,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ed6bd186-8d8e-4ffb-95a0-5ed81f6640ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(pred, true, target_names=None, output_dict=False):\n",
    "    return classification_report(\n",
    "        true,\n",
    "        pred,\n",
    "        target_names=target_names,\n",
    "        output_dict=output_dict,\n",
    "        zero_division=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5cf59322-a4d2-47aa-8e57-3328bbb10e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = './OUTPUT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4d398f44-64e7-4fd5-91f4-a9451b33ed1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 0 batch: 0 loss: 1.42933 train_acc: 0.2 dev_acc: 0.4\n",
      ">> epoch: 0 batch: 500 loss: 0.94256 train_acc: 0.8 dev_acc: 0.7\n",
      ">> epoch: 0 batch: 1000 loss: 0.44073 train_acc: 0.7 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 1500 loss: 0.57633 train_acc: 0.7 dev_acc: 0.8\n",
      ">> epoch: 0 batch: 2000 loss: 0.66611 train_acc: 0.6 dev_acc: 0.7\n",
      ">> epoch: 0 batch: 2500 loss: 0.35223 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 3000 loss: 0.29196 train_acc: 0.8 dev_acc: 0.8\n",
      ">> epoch: 0 batch: 3500 loss: 0.2984 train_acc: 0.9 dev_acc: 0.7\n",
      ">> epoch: 0 batch: 4000 loss: 0.65905 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 0 batch: 4500 loss: 0.32991 train_acc: 0.8 dev_acc: 1.0\n",
      ">> epoch: 0 batch: 5000 loss: 1.00685 train_acc: 0.7 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 5500 loss: 0.13897 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 6000 loss: 0.14716 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 6500 loss: 0.44635 train_acc: 0.8 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 7000 loss: 0.18137 train_acc: 1.0 dev_acc: 1.0\n",
      ">> epoch: 0 batch: 7500 loss: 0.86027 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 8000 loss: 0.49385 train_acc: 0.9 dev_acc: 0.5\n",
      ">> epoch: 0 batch: 8500 loss: 0.10867 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 0 batch: 9000 loss: 0.24219 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 0 batch: 9500 loss: 0.04368 train_acc: 1.0 dev_acc: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [05:35<11:11, 335.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 1 batch: 0 loss: 0.31532 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 500 loss: 0.04972 train_acc: 1.0 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 1000 loss: 1.03451 train_acc: 0.7 dev_acc: 0.6\n",
      ">> epoch: 1 batch: 1500 loss: 0.6972 train_acc: 0.7 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 2000 loss: 0.08108 train_acc: 1.0 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 2500 loss: 0.26872 train_acc: 0.9 dev_acc: 0.7\n",
      ">> epoch: 1 batch: 3000 loss: 0.40997 train_acc: 0.8 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 3500 loss: 0.18261 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 4000 loss: 0.22237 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 4500 loss: 0.67195 train_acc: 0.7 dev_acc: 0.7\n",
      ">> epoch: 1 batch: 5000 loss: 0.65606 train_acc: 0.8 dev_acc: 1.0\n",
      ">> epoch: 1 batch: 5500 loss: 0.10515 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 6000 loss: 0.2915 train_acc: 0.8 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 6500 loss: 1.08978 train_acc: 0.7 dev_acc: 1.0\n",
      ">> epoch: 1 batch: 7000 loss: 0.05594 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 7500 loss: 0.08123 train_acc: 1.0 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 8000 loss: 0.38626 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 1 batch: 8500 loss: 0.28378 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 1 batch: 9000 loss: 0.27301 train_acc: 0.8 dev_acc: 1.0\n",
      ">> epoch: 1 batch: 9500 loss: 0.72837 train_acc: 0.7 dev_acc: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [11:06<05:32, 332.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch: 2 batch: 0 loss: 0.09919 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 500 loss: 0.06005 train_acc: 1.0 dev_acc: 1.0\n",
      ">> epoch: 2 batch: 1000 loss: 0.06538 train_acc: 1.0 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 1500 loss: 0.2412 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 2000 loss: 0.55536 train_acc: 0.7 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 2500 loss: 0.23445 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 2 batch: 3000 loss: 0.08402 train_acc: 1.0 dev_acc: 1.0\n",
      ">> epoch: 2 batch: 3500 loss: 0.70136 train_acc: 0.8 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 4000 loss: 0.35669 train_acc: 0.8 dev_acc: 1.0\n",
      ">> epoch: 2 batch: 4500 loss: 0.15589 train_acc: 0.9 dev_acc: 0.7\n",
      ">> epoch: 2 batch: 5000 loss: 0.10645 train_acc: 1.0 dev_acc: 0.8\n",
      ">> epoch: 2 batch: 5500 loss: 0.66848 train_acc: 0.8 dev_acc: 1.0\n",
      ">> epoch: 2 batch: 6000 loss: 0.48002 train_acc: 0.8 dev_acc: 0.7\n",
      ">> epoch: 2 batch: 6500 loss: 0.56361 train_acc: 0.7 dev_acc: 0.8\n",
      ">> epoch: 2 batch: 7000 loss: 0.11765 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 2 batch: 7500 loss: 0.11578 train_acc: 0.9 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 8000 loss: 0.17192 train_acc: 0.9 dev_acc: 0.8\n",
      ">> epoch: 2 batch: 8500 loss: 0.13495 train_acc: 0.9 dev_acc: 1.0\n",
      ">> epoch: 2 batch: 9000 loss: 0.25398 train_acc: 0.8 dev_acc: 0.9\n",
      ">> epoch: 2 batch: 9500 loss: 0.1983 train_acc: 1.0 dev_acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [16:36<00:00, 332.23s/it]\n"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(EPOCH)):\n",
    "    for b, (input, mask, target) in enumerate(train_loader):\n",
    "        input = input.to(DEVICE)\n",
    "        mask = mask.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "\n",
    "        pred = model(input, mask)\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if b % 500 != 0:\n",
    "            continue\n",
    "\n",
    "        y_pred = torch.argmax(pred, dim=1)\n",
    "        report = evaluate(y_pred.cpu().data.numpy(), target.cpu().data.numpy(), output_dict=True)\n",
    "        with torch.no_grad():\n",
    "            for dev_input, dev_mask, dev_target in dev_loader:\n",
    "                dev_input = dev_input.to(DEVICE)\n",
    "                dev_mask = dev_mask.to(DEVICE)\n",
    "                dev_target = dev_target.to(DEVICE)\n",
    "                dev_pred = model(dev_input, dev_mask)\n",
    "                dev_pred_ = torch.argmax(dev_pred, dim=1)\n",
    "                dev_report = evaluate(dev_pred_.cpu().data.numpy(), dev_target.cpu().data.numpy(), output_dict=True)\n",
    "                break\n",
    "        print(\n",
    "            '>> epoch:', e,\n",
    "            'batch:', b,\n",
    "            'loss:', round(loss.item(), 5),\n",
    "            'train_acc:', report['accuracy'],\n",
    "            'dev_acc:', dev_report['accuracy']\n",
    "        )\n",
    "    if e%50 ==0:\n",
    "        torch.save(model, MODEL_DIR + f'{e}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08238f3-6168-4ab5-aae7-4d6f90c989bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
