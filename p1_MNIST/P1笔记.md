>- **🍨 本文为[🔗365天深度学习训练营](https://mp.weixin.qq.com/s/xLjALoOD8HPZcH563En8bQ) 中的学习记录博客**
>- **🍦 参考文章：[365天深度学习训练营-第P1周：实现mnist手写数字识别](https://mp.weixin.qq.com/s/00Z7negmsNuYSHY6nqeEWA)**
>- **🍖 原作者：[K同学啊|接辅导、项目定制](https://mtyjkh.blog.csdn.net/)**

torch.utils.data.DataLoader:
这个地方是对于Dataset—
● dataset(string) ：加载的数据集
● batch_size (int,optional) ：每批加载的样本大小（默认值：1）
● shuffle(bool,optional) : 如果为True，每个epoch重新排列数据。
● sampler (Sampler or iterable, optional)  ： 定义从数据集中抽取样本的策略。 可以是任何实现了 `__len__` 的 Iterable。 如果指定，则不得指定 shuffle  。
● batch_sampler (Sampler or iterable, optional) ： 类似于sampler，但一次返回一批索引。与 batch_size、shuffle、sampler 和 drop_last 互斥。
● num_workers(int,optional) ： 用于数据加载的子进程数。 0 表示数据将在主进程中加载（默认值：0）。
● pin_memory (bool,optional) :  如果为 True，数据加载器将在返回之前将张量复制到设备/CUDA 固定内存中。 如果数据元素是自定义类型，或者collate_fn返回一个自定义类型的批次。
● drop_last(bool,optional) :  如果数据集大小不能被批次大小整除，则设置为 True 以删除最后一个不完整的批次。 如果 False 并且数据集的大小不能被批大小整除，则最后一批将保留。 （默认值：False）  
● timeout(numeric,optional) :  设置数据读取的超时时间 ， 超过这个时间还没读取到数据的话就会报错。（默认值：0）  
● worker_init_fn(callable,optional) ： 如果不是 None，这将在步长之后和数据加载之前在每个工作子进程上调用，并使用工作 id（[0，num_workers - 1] 中的一个 int）的顺序逐个导入。 （默认：None） 


**需要注意的是，这里需要手动设置batch_size**

```python
def test (dataloader, model, loss_fn):
    size        = len(dataloader.dataset)  # 测试集的大小，一共10000张图片
    num_batches = len(dataloader)          # 批次数目，313（10000/32=312.5，向上取整）
    test_loss, test_acc = 0, 0
    
    # 当不进行训练时，停止梯度更新，节省计算内存消耗
    with torch.no_grad():
        for imgs, target in dataloader:
            imgs, target = imgs.to(device), target.to(device)
            
            # 计算loss
            target_pred = model(imgs)
            loss        = loss_fn(target_pred, target)
            
            test_loss += loss.item()
            test_acc  += (target_pred.argmax(1) == target).type(torch.float).sum().item()

    test_acc  /= size
    test_loss /= num_batches

    return test_acc, test_loss
```
**在这个测试函数当中，需要注意torch.no_grad的用法**
with torch.no_grad的作用
在该模块下，所有计算得出的tensor的requires_grad都自动设置为False。

即使一个tensor（命名为x）的requires_grad = True，在with torch.no_grad计算，由x得到的新tensor（命名为w-标量）requires_grad也为False，且grad_fn也为None,即不会对w求导。


**在正式训练当中，注意设置model.trian()和model.eval(),避免模型参数的更新**
1. model.train()
model.train()的作用是启用 Batch Normalization 和 Dropout。

如果模型中有BN层(Batch Normalization）和Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。

2. model.eval()
model.eval()的作用是不启用 Batch Normalization 和 Dropout。

如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。

训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。


>问题：
在test函数中，已经设置了with torch.no_grad(),按理说已经不会有梯度产生了，为什么还需要设置model.eval（）呢？